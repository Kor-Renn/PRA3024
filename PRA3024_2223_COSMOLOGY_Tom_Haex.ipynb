{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kor-Renn/PRA3024/blob/main/PRA3024_2223_COSMOLOGY_Tom_Haex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkdruF2bYl5j"
      },
      "source": [
        "# <font color='deepskyblue'>Cosmology MCMC notebook </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0SazaVif4pl"
      },
      "source": [
        "## <font color='deepskyblue'>Preliminaries</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6hcXmMbhbMA",
        "outputId": "0ebc568f-db70-410a-9139-084cea26495f",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: getdist in /usr/local/lib/python3.9/dist-packages (1.4.3)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.9/dist-packages (from getdist) (1.10.1)\n",
            "Requirement already satisfied: matplotlib!=3.5.0,>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from getdist) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.9/dist-packages (from getdist) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from getdist) (23.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.9/dist-packages (from getdist) (6.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.5.0,>=2.2.0->getdist) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.5.0,>=2.2.0->getdist) (4.39.3)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.5.0,>=2.2.0->getdist) (5.12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.5.0,>=2.2.0->getdist) (2.8.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.5.0,>=2.2.0->getdist) (1.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.5.0,>=2.2.0->getdist) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.5.0,>=2.2.0->getdist) (3.0.9)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.5.0,>=2.2.0->getdist) (8.4.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib!=3.5.0,>=2.2.0->getdist) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib!=3.5.0,>=2.2.0->getdist) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages (if not available on server)\n",
        "!pip install getdist\n",
        "\n",
        "# Load required packages\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.integrate as integrate\n",
        "# import pylab\n",
        "import getdist, IPython\n",
        "from numpy import random\n",
        "# from IPython.display import Image\n",
        "from getdist import plots, MCSamples, loadMCSamples\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import sympy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayxOF90lVqKY"
      },
      "source": [
        "\n",
        "<font color='tomato'>\n",
        "(NB: The equations you'll directly need to build your code are those $\\boxed{\\text{with a box around}}$)\n",
        "</font>\n",
        "\n",
        "## <font color='deepskyblue'>The $\\Lambda$CDM Model</font>\n",
        "\n",
        "The Lambda-Cold-Dark-Matter ($\\Lambda$CDM) Model is our concordance cosmological model. It postulates a Universe that behaves according to **General Relativity**, filled with **baryons** (protons, neutrons - the stuff we and stars are made of!), **Cold Dark Matter (CDM)** and a **Cosmological Constant** $\\Lambda$.\n",
        "\n",
        "The expansion of the Universe is best described in terms of the **scale factor** $a$ (the \"radius\" of the Universe, conventionally set to 1 today) and the **Hubble rate** $H$\n",
        "\n",
        "$$\n",
        "H \\equiv \\frac{\\dot a}{a}\n",
        "$$\n",
        "\n",
        "The Einstein Equations in a **homogeneous, isotropic** Universe have an especially simple form and a special name, **Friedmann Equation**:\n",
        "\n",
        "$$\n",
        "H^2 = \\frac{8\\pi G}{3}\\rho + \\frac{k}{a^2}\n",
        "$$\n",
        "\n",
        "where $\\rho$ is the total energy density of the Universe (at a certain time or size), and $k$ is a universal constant related to the [spatial curvature and shape of the Universe](https://en.wikipedia.org/wiki/Shape_of_the_universe#:~:text=The%20spatial%20curvature%20is%20related,indistinguishable%20spaces%20with%20different%20topologies.). In the $\\Lambda$CDM model (and at late enough times), this additionally simplifies to\n",
        "\n",
        "$$\n",
        "\\boxed{\n",
        "H^2 = H_0^2\\left(\\Omega_\\Lambda + \\Omega_m a^{-3} + \\Omega_k a^{-2}\\right)\n",
        "}\n",
        "\\qquad\n",
        "\\qquad\n",
        "\\boxed{\n",
        "\\Omega_m + \\Omega_\\Lambda + \\Omega_k = 1\n",
        "}\n",
        "\\tag{1}\n",
        "$$\n",
        "\n",
        "$H_0$ is the value of the Hubble rate *today*, and the parameters $\\Omega_m$, $\\Omega_\\Lambda$ and $\\Omega_k$ are dimensionless quantities indicating how much of the present expansion of the Universe is due to (non-relativistic) matter, cosmological constant $\\Lambda$, and spatial curvature $k$. Note that the three $\\Omega$'s are *not independent* (their sum is 1) and that we grouped together the contribution of baryons and CDM,\n",
        "\n",
        "$$\n",
        "\\Omega_m = \\Omega_{\\rm CDM} + \\Omega_{\\rm b}\n",
        "$$\n",
        "\n",
        "and that we have evidence that independent evidence that $\\Omega_b \\approx 5$\\%. Note that only $\\Omega_m \\geq 0$ makes physical sense, while in principle both $\\Omega_\\Lambda$ and $\\Omega_k$ can be negative.\n",
        "\n",
        "The present Hubble rate is conventionally parametrised in terms of the dimensionless $h$:\n",
        "\n",
        "$$\n",
        "\\boxed{H_0 \\equiv h\\,\\frac{100 \\,\\text{km}}{\\text{s Mpc}}}\n",
        "\\tag{2}\n",
        "$$\n",
        "\n",
        "where Mpc = Megaparsec and [parsec](https://en.wikipedia.org/wiki/Parsec) equals to about $3.26$ light years or $3.086 \\cdot 10^{16}$ m. \n",
        "\n",
        "In essence, these equations give us the relation between *what exists in the Universe* and *how the Universe expanded*. Measure one, I can tell you the other. This is precisely what we aim to do!\n",
        "\n",
        "Time to define the function $H(z)$. Don't forget to include the useful parameters as arguments of the function, and to define `parsec`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFIYCrH2XccJ",
        "outputId": "0464c2fc-6352-45cd-c92d-30f607c6bcfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10509100.0, 1.1667444788088233e-07)\n"
          ]
        }
      ],
      "source": [
        "# Hubble rate: look at equations (1) and (2) for the definition!\n",
        "def H(z, OmegaL, OmegaM):\n",
        "  # H**2 = H0**2( ohmlambda + ohmmatter*Z**3 + ohmk*z**2) == H0**2(Omega)  where Omega is the z-adjusted sum of these omegas.\n",
        "  OmegaK = 1 - OmegaL - OmegaM\n",
        "  Omega = OmegaL + OmegaM * (1+z)**3 + OmegaK * (1+z)**2\n",
        "  return Omega\n",
        "\n",
        "# Integrate H from x = 0 to x = z\n",
        "def D(z, omegaL, omegaM):\n",
        "  return integrate.quad(lambda x: H(x, omegaL, omegaM), 0, z)\n",
        "\n",
        "print(D(100, 0.3, 0.4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNm-m0auXccK"
      },
      "source": [
        "## <font color='deepskyblue'>Supernovae Ia</font>\n",
        "\n",
        "In order to track the expansion of the Universe, we will look at Supernovae type Ia observations. SNIa are formed when a *white dwarf* accretes matter from a companion until it reaches a critical mass of about $1.44 M_\\odot$ (solar masses), after which it explodes **at always the same luminosity** $L_{\\rm SN}$. Therefore, the observed [magnitude](https://en.wikipedia.org/wiki/Magnitude_(astronomy)) is only a function of their [**luminosity distance**](https://en.wikipedia.org/wiki/Distance_measures_(cosmology)) from Earth, defined as the ratio of the intrinsic luminosity $L_{\\rm SNIa}$ and the measured *surface brightness* $\\Phi$ (energy per unit time per unit area) \n",
        "\n",
        "$$ d_L\\equiv \\frac{L_{\\rm SN}}{4\\pi \\Phi} $$\n",
        "\n",
        "While photons (light rays) travel from the distant SNIa to Earth, the Universe expands and the photons get redshifted, and this redshift is directly related to the size of the Universe when the photon was emitted compared to today:\n",
        "\n",
        "$$ \\frac{\\lambda_{\\rm obs}}{\\lambda_{\\rm em}} \\equiv 1 + z_{\\rm em} = \\frac{1}{a_{\\rm em}}$$\n",
        "\n",
        "We can express the luminosity distance in terms of the Hubble rate:\n",
        "\n",
        "$$\n",
        "\\boxed{\n",
        "d_L(z) = \\frac{c}{H_0}(1+z) \\times \n",
        "\\begin{cases}\n",
        "    \\cfrac{1}{\\sqrt{\\Omega_k}} \\,\\sinh\\left(\\sqrt{\\Omega_k}\\,D(z)\\right) & \\Omega_k > 0\n",
        "    \\\\[.5em]\n",
        "    D(z) & \\Omega_k = 0\n",
        "    \\\\[.5em]\n",
        "    \\cfrac{1}{\\sqrt{-\\Omega_k}} \\,\\sin\\left(\\sqrt{-\\Omega_k}\\,D(z)\\right) & \\Omega_k < 0\n",
        "\\end{cases}\n",
        "\\qquad\\qquad\n",
        "D(z) \\equiv H_0\\int_0^z \\frac{{\\rm d}Z}{H(Z)}\n",
        "}\n",
        "\\tag{3}\n",
        "$$\n",
        "\n",
        "where $c$ is the speed of light: $c \\simeq 2.998 \\cdot 10^{5}$ km/s.\n",
        "\n",
        "Clearly, there exists a relation between $d_L(z)$ and $H(z)$, so the luminosity distance is a useful measure of the Universe expansion history. \n",
        "In practice, however, the actual observable for [Supernovae Type Ia](https://en.wikipedia.org/wiki/Type_Ia_supernova) (or SNIa) is the [distance modulus](https://en.wikipedia.org/wiki/Distance_modulus) $\\mu$, defined as the difference between the *apparent magnitude* $m$ and the *absolute magnitude* $M$:\n",
        "\n",
        "$$\n",
        "\\boxed{\n",
        "\\mu(z) \\equiv m(z) - M \\equiv 5 \\log_{10} \\frac{d_L(z)}{10\\,\\text{pc}}\n",
        "}\n",
        "\\tag{4}\n",
        "$$\n",
        "\n",
        "We can create the function `dL(z, h, Omega_m, Omega_Lambda)`. You can also pass the function `Hubble` as an argument: `dL(z, Hubble, Omega_m, Omega_Lambda)`. Additionally, you can create $D(z)$ for your convenience.\n",
        "\n",
        "Note that you need to integrate the function $H(z)$, so write your own quick-and-easy numerical integrator, for example implementing the [trapezoidal rule](https://en.wikipedia.org/wiki/Trapezoidal_rule) or your favourite alternative, or [look up the pre-written methods](https://docs.scipy.org/doc/scipy/tutorial/integrate.html). Make sure it returns $d_L$ in parsecs and that it uses $H_0$ measured in units of 100 km/s/Mpc (which we called $h$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "QAwegy3UXccK"
      },
      "outputs": [],
      "source": [
        "SPEED_OF_LIGHT = 2.99792458*100000000 # (3.0*10^8) metres per second\n",
        "Parsec = 3.086*10000000000000000 # 10^16 metres\n",
        "MParsec = Parsec * 1000000\n",
        "\n",
        "def dL(z, h, OmegaM, OmegaL):\n",
        "  OmegaK = 1-OmegaM - OmegaL\n",
        "  H0 = h * (100 *10**3)/MParsec\n",
        "  Dz = H0 * integrate.quad(lambda x: 1/H(x, OmegaL, OmegaM), 0, z)\n",
        "  term1 = (1+z)*(SPEED_OF_LIGHT) / H0\n",
        "\n",
        "  if OmegaK == 0:\n",
        "    term2 = Dz\n",
        "  elif OmegaK < 0:\n",
        "    term2 = math.sin(math.sqrt(-OmegaK)*Dz)/math.sqrt(-OmegaK)\n",
        "  else: #OmegaK must be greater than 0 to get here\n",
        "    term2 = math.sinh(math.sqrt(OmegaK)*Dz)/math.sqrt(Omegak)\n",
        "    dl = term1 * term2\n",
        "    dl = dl/Parsec\n",
        "    #dl is now no longer in metres but in parsecs\n",
        "  return dl\n",
        "\n",
        "def mu(z, h, OmeagaM, OmegaL):\n",
        "  dl = dL(z, h, OmegaM, OmegaL)\n",
        "  Dl = dl/(10*Parsec)\n",
        "  return 5*log10(Dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLWjCnSSXccL"
      },
      "source": [
        "## <font color='deepskyblue'>The Union2.1 Compilation</font>\n",
        "\n",
        "Our source of data is from [Union2.1](http://supernova.lbl.gov/Union/). It is a collection of $580$ SNIa, each observation is comprised of redshift $z$ \n",
        "Our goal today is to use SNIa measurements to constrain the parameters ($h, \\Omega_\\Lambda, \\Omega_m$).\n",
        "\n",
        "In essence, we can calculate for each redshift the theoretical value of $\\mu$ using this equation with the expression for $d_L(z)$ and our model of choice $H(z)$ -- note that we will need to perform a numerical integral! -- and compare this value with observations. A different model would amount to changing the theoretical value of $H(z)$.\n",
        "\n",
        "Let us start by importing and plotting the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKb3QGqXTuJ5",
        "outputId": "600a6521-f373-4047-fff9-70f5ba92f9d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[           nan 2.84880000e-02 3.53465834e+01 2.23905933e-01\n",
            " 1.28418942e-01]\n"
          ]
        }
      ],
      "source": [
        "dataloc = \"http://supernova.lbl.gov/Union/figures/SCPUnion2.1_mu_vs_z.txt\"\n",
        "\n",
        "# Read in data as numpy array.\n",
        "# Format is [name, redshift, magnitude, magnitude error, and another number?]\n",
        "data = np.genfromtxt(dataloc)\n",
        "\n",
        "# Print the first line as an example. (Note that genfromtxt turns the names into 'Not A Number')\n",
        "print(data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "2mvAA83gVIHz"
      },
      "outputs": [],
      "source": [
        "# Extract the redshifts (zs), distance modulus (dist_mod) and magnitude estimated errors (err_dist_mod)\n",
        "zs = data.T[1] # redshift\n",
        "distance_modulus = data.T[2] # distance modulus\n",
        "error_distance_modulus = data.T[3] # errors on distance modulus = sigmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "x2hF3drbVPEW"
      },
      "outputs": [],
      "source": [
        "# Let's look at the data, just for fun. \n",
        "# We can look both at the raw data (distance modulus) or calculate the luminosity distance\n",
        "# Note that Numpy allows us to manipulate whole arrays at once with something like:\n",
        "#     array_2 = some_function_of(array_1)\n",
        "\n",
        "# luminosity distance in pc\n",
        "luminosity_distance_pc = 10. * 10.**(distance_modulus / 5.)\n",
        "\n",
        "# and in Mpc\n",
        "luminosity_distance_Mpc = luminosity_distance_pc / 10.**6\n",
        "\n",
        "# and the error on that distance:\n",
        "error_luminosity_distance_Mpc = 5. * (10.**((distance_modulus + error_distance_modulus)/5.-6.) - 10.**((distance_modulus - error_distance_modulus)/5.-6.))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "o81D-kOYYwx5"
      },
      "outputs": [],
      "source": [
        "plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot distance modulus versus redshift (original data)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.xlabel(\"Redshift\")\n",
        "plt.ylabel(\"Distance Modulus\")\n",
        "plt.errorbar(zs, distance_modulus, yerr=error_distance_modulus, fmt='o', color='black', ecolor='darkcyan', markersize=4)\n",
        "\n",
        "# Plot luminosity distance versus redshift\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.xlabel(\"Redshift\")\n",
        "plt.ylabel(\"Luminosity Distance [Mpc]\")\n",
        "plt.errorbar(zs, luminosity_distance_Mpc, yerr=error_luminosity_distance_Mpc, fmt='o', color='black', ecolor='darkcyan', markersize=4)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hxy2SP9SXccN"
      },
      "source": [
        "## <font color='deepskyblue'>Bayes' Theorem: Likelihood, Prior, Posterior (and Evidence...)</font>\n",
        "\n",
        "The **conditional probability of A given that B is true** is denoted with $P(A|B)$ (read: *probability of A given B*). \n",
        "Did A happen? Then B has probability $P(B|A)$ of happening.\n",
        "\n",
        "[**Bayes' Theorem**](https://en.wikipedia.org/wiki/Bayes%27_theorem) is simply a reflection of the symmetry of the concept of joint probability:\n",
        "\n",
        "$$\n",
        "P(A|B)P(B) = P(B|A)P(A)\n",
        "$$\n",
        "\n",
        "In physics, our goal is often to determine how likely the values of some parameters $\\theta$ are, given how well they reproduce the data $D$. \n",
        "We write Bayes' theorem in the following equivalent form:\n",
        "\n",
        "$$\n",
        "P(\\theta | D) = \\frac{P(D|\\theta)P(\\theta)}{P(D)}\n",
        "$$\n",
        "\n",
        "The various terms conventionally take the names:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "P(\\theta | D) & && \\text{Posterior}\n",
        "\\\\\n",
        "P(D | \\theta)& && \\text{Likelihood}\n",
        "\\\\\n",
        "P(\\theta) & && \\text{Prior}\n",
        "\\\\\n",
        "P(D) & && \\text{Evidence}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "### <font color='deepskyblue'>Prior</font>\n",
        "\n",
        "The *prior* quantifies our degree of prior belief in the model parameters. It can contain the results of previous experiments, or theoretical considerations (for instance, that certain parameters cannot be negative, etc.). \n",
        "\n",
        "We can have *strong priors* about something - the Sun still existing at night, see the comic below - or not.\n",
        "When we do *not* have compelling prior knowledge about a parameter, we generally use a *non-informative* prior probability distribution, most often related to the [uniform distribution](https://en.wikipedia.org/wiki/Continuous_uniform_distribution)\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    \\text{Uniform} & && \\theta \\in \\text{Uniform}[\\theta_{\\rm min}, \\theta_{\\rm max}]\n",
        "    \\\\\n",
        "    \\text{Log Uniform} & && \\log\\theta \\in \\text{Uniform}[\\log\\theta_{\\rm min}, \\log\\theta_{\\rm max}]\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "You may sometimes hear that \"the prior doesn't matter\". While this is not strictly true, it is however true that in many instances the *likelihood* depends more dramatically on the parameters and will have a dominating contribution to the posterior. You are more than welcome to try different priors for the model parameters and see if and how your results change.\n",
        "\n",
        "### <font color='deepskyblue'>Likelihood</font>\n",
        "\n",
        "The measure of how well our model reproduces the data is the **likelihood** $\\mathcal L$. For normal (=Gaussian), uncorrelated data $\\mu_i$ with errors $\\sigma_i$ ($i = \\{1,580\\}$), the likelihood is related to the famous [**chi-squared**](https://en.wikipedia.org/wiki/Chi-square_distribution) via\n",
        "\n",
        "$$\n",
        "P(D | \\theta) \\equiv \\mathcal L = \\exp\\left(-\\frac{\\chi^2}{2}\\right)\n",
        "\\qquad\n",
        "\\qquad\n",
        "\\boxed{\n",
        "\\chi^2 \\equiv \\sum_{i=1}^{580} \\frac{\\left(\\mu_i^{(\\rm theor)} - \\mu_i^{(\\rm obs)}\\right)^2}{\\sigma_i^2}\n",
        "}\n",
        "\\tag{5}\n",
        "$$\n",
        "\n",
        "A *high likelihood* corresponds to a *low chi squared*.\n",
        "\n",
        "In principle, we could attack the problem brute-force: create a grid of values for $h$, $\\Omega_m$, $\\Omega_\\Lambda$, calculate the likelihood for each combination, and create a profile for the likelihood. \n",
        "\n",
        "In practical cases, however, this is not only sub-optimal but sometimes **impossible**. Think of complicated models with 20+ free parameters (this is not that unusual in physics, by the way). Even a very coarse grained grid of 100 values for each parameter would result in more than $10^{40}$ (!!!) different parameter combination to be tested. You can probably understand why this is undesirable.\n",
        "\n",
        "### <font color='deepskyblue'>Evidence</font>\n",
        "\n",
        "This is the tricky bit. The evidence, which is \"*the probability of observing certain data*\", is not something that is easy to evaluate or interpret. This looks like a serious limitation to finding the:\n",
        "\n",
        "\n",
        "### <font color='deepskyblue'>Posterior</font>\n",
        "\n",
        "What does the data tell us about the model parameters? The answer is the *posterior probability distribution*, often (but not necessarily) expressed as (average $\\pm$ standard deviation)\n",
        "\n",
        "$$\n",
        "\\theta_i = \\bar\\theta_i \\pm \\sigma_i\n",
        "$$\n",
        "\n",
        "In other words, the posterior tells us how likely it is for the model parameters to have certain values, *given that we observe the data* $D$.\n",
        "\n",
        "*This is the final goal of our analysis!*\n",
        "\n",
        "<br>\n",
        "\n",
        "![title](http://1.bp.blogspot.com/-tTx4HDejSww/ULcTeK9_V_I/AAAAAAAAA8g/PCYOH-eXGbk/s1600/frequentists_vs_bayesians.png)\n",
        "![title](https://i.redd.it/5r0hwixt0m931.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lymI_LaObhcU"
      },
      "source": [
        "## <font color='deepskyblue'>Markov-Chain Monte Carlo</font>\n",
        "\n",
        "Markov-Chain Monte Carlo techniques are a class of methods to sample a probability distribution $P(x)$ - in our case $P(\\theta | D)$ - provided that we can calculate a density $\\mathcal F(x)$ **that is proportional to it** - in our case $P(\\theta)P(D|\\theta)$ = likelihood $\\times$ prior. Among these, a very popular one is the [**Metropolis-Hastings**](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm) algorithm, which is what we will use.\n",
        "\n",
        "Schematically, the algorithm works as follows:\n",
        "* Start with an $n$-tuple of parameters $\\theta_{\\rm old} = \\{\\theta_1, \\theta_2, \\dots\\}_{\\rm old}$ (in our case $\\theta = \\{h,\\Omega_m,\\Omega_\\Lambda\\}$) and calculate the $\\mathcal F_{\\rm old}$\n",
        "* Propose a new set of parameters $\\theta_{\\rm prop}$, calculate the new corresponding $\\mathcal F_{\\rm prop}$. How you propose the new parameters is not essential as long as the probability of going from one set of parameters to the next is the same as the inverse: $P(\\theta_a \\to \\theta_b) = P(\\theta_b \\to \\theta_a)$. For this, we normally take Gaussian displacements with some (relatively small) standard deviation.\n",
        "* Compare the new and old likelihoods-priors:\n",
        "    * If $\\mathcal F_{\\rm prop} > \\mathcal F_{\\rm old}$, accept the step: $\\theta_{\\rm new} = \\theta_{\\rm prop}$\n",
        "    * Otherwise, accept with probability $P_{\\rm accept} = \\mathcal F_{\\rm prop}/\\mathcal F_{\\rm old}$\n",
        "* If accepted, $\\theta_{\\rm new} = \\theta_{\\rm prop}$ and record the step; otherwise, $\\theta_{\\rm new} = \\theta_{\\rm old}$ (no re-recording)\n",
        "* Update $\\theta_{\\rm old} = \\theta_{\\rm new}$\n",
        "* Repeat\n",
        "\n",
        "The MC chain keeps going until the desired number of steps or, ideally, when a suitable convergence test, like for example the [Gelman-Rubin test](https://bookdown.org/rdpeng/advstatcomp/monitoring-convergence.html), is satisfied. \n",
        "Today we will just set a limit to the total number of steps, but please feel free to implement and test your favourite convergence test!\n",
        "\n",
        "Looking at equation (5), you can convince yourself that\n",
        "$$\n",
        "\\frac{\\mathcal L_a}{\\mathcal L_b} = \\exp\\left(\\frac{\\chi_b^2 - \\chi_a^2}{2}\\right)\n",
        "$$\n",
        "\n",
        "(This is useful because $\\chi^2$ can be large, so we want to avoid the likelihoods evaluating to `0.` -- computers don't like dividing by zero, unfortunately)\n",
        "\n",
        " \n",
        "* <font color=\"deepskyblue\"> Start with an $n$-tuple of parameters $\\theta_{\\rm old} = \\{\\theta_1, \\theta_2, \\dots\\}_{\\rm old}$ (in our case $\\theta = \\{h,\\Omega_m,\\Omega_\\Lambda\\}$) and the corresponding chi squared $\\chi_{\\rm old}^2$ (to be clear, for us this implies calculating a series of 580 integrals and comparing each with the observed values) </font>\n",
        "* <font color=\"deepskyblue\"> Propose new parameters $\\theta_{\\rm prop}$ and calculate the corresponding chi squared $\\chi_{\\rm prop}^2$</font>\n",
        "* <font color=\"deepskyblue\"> Draw a random number $r \\in [0,1]$</font>\n",
        "* <font color=\"deepskyblue\"> Accept the step if \n",
        "$$\n",
        "r < \\exp\\left(\\frac{\\chi_{\\rm old}^2 - \\chi_{\\rm prop}^2}{2}\\right) \\frac{P(\\theta)_{\\rm prop}}{P(\\theta)_{\\rm old}}\n",
        "$$</font>\n",
        "    * <font color=\"deepskyblue\"> If accepted, update $\\theta_{\\rm old} = \\theta_{\\rm prop}$, and record; otherwise, $\\theta_{\\rm new} = \\theta_{\\rm old}$ (no re-recording)</font>\n",
        "    * <font color=\"deepskyblue\">If you want, you can keep a record of the best fit (lowest $\\chi^2$), but this is not the specific purpose of MCMC! We are exploring the parameter space and sampling a _posterior distribution_, not just finding the best fit</font>\n",
        "* <font color=\"deepskyblue\"> Repeat</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "xvF9UhsMXccR",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "6b48da06-2d29-45d5-84d7-6f0af9b42556"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-8f80dd826ed2>\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m#and then now to get started proper:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mChiold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchi2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-8f80dd826ed2>\u001b[0m in \u001b[0;36mchi2\u001b[0;34m(h, OmegaM, OmegaL)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#and now this allows for a pretty easy CHI2 function:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mchi2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOmegaM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOmegaL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectormu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mOmegaM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOmegaL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'distance_modulus'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'distance_modulus_error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#this method changes the parameters up a bit so a change can be measured.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-8f80dd826ed2>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(h, OmegaM, OmegaL)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#followed by a reccomendation to shorten it even further:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mvectormu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOmegaM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOmegaL\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'redshift'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOmegaM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOmegaL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOmega_M\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOmega_L\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2302\u001b[0m             \u001b[0mvargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_n\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_n\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2304\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vectorize_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_ufunc_and_otypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_vectorize_call\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2380\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2382\u001b[0;31m             \u001b[0mufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0motypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ufunc_and_otypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m             \u001b[0;31m# Convert args to object arrays first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_get_ufunc_and_otypes\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2341\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2342\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2344\u001b[0m             \u001b[0;31m# Performance note: profiling indicates that -- for simple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-7cbe58e98a67>\u001b[0m in \u001b[0;36mmu\u001b[0;34m(z, h, OmeagaM, OmegaL)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOmeagaM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOmegaL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOmegaM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOmegaL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m   \u001b[0mDl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mParsec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-7cbe58e98a67>\u001b[0m in \u001b[0;36mdL\u001b[0;34m(z, h, OmegaM, OmegaL)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mOmegaK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mOmegaM\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mOmegaL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mH0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mMParsec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mDz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mintegrate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOmegaL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOmegaM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mterm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSPEED_OF_LIGHT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mH0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/scipy/integrate/_quadpack_py.py\u001b[0m in \u001b[0;36mquad\u001b[0;34m(func, a, b, args, full_output, epsabs, epsrel, limit, points, weight, wvar, wopts, maxp1, limlst, complex_func)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         retval = _quad(func, a, b, args, full_output, epsabs, epsrel, limit,\n\u001b[0m\u001b[1;32m    464\u001b[0m                        points)\n\u001b[1;32m    465\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/scipy/integrate/_quadpack_py.py\u001b[0m in \u001b[0;36m_quad\u001b[0;34m(func, a, b, args, full_output, epsabs, epsrel, limit, points)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpoints\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minfbounds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_quadpack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qagse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfull_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsabs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsrel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_quadpack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qagie\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbound\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minfbounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfull_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsabs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsrel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
          ]
        }
      ],
      "source": [
        "filename = \"MC_Chain_SNIa.dat\" # Or choose your favourite filename, obviously\n",
        "file = open(filename,\"w\")\n",
        "\n",
        "# Start from some very bad values, to show that the MCMC works anyway!\n",
        "Omega_M = 0. # \n",
        "Omega_L = 0.\n",
        "h = 0.1\n",
        "\n",
        "#in order to be make everything easier to manipulate, meaning less complicated code is needed, a dataframe is created.\n",
        "dataframe = pd.DataFrame({\"redshift\":zs, \n",
        "                   \"distance_modulus\":distance_modulus,\n",
        "                   \"distance_modulus_error\":error_distance_modulus, \n",
        "                   \"Luminosity distance pc\":luminosity_distance_pc,\n",
        "                   \"luminosity_distance_Mpc\":luminosity_distance_Mpc ,\n",
        "                   \"error_luminosity_distance_Mpc\":error_luminosity_distance_Mpc})\n",
        "dataframe.describe()\n",
        "\n",
        "#a reccomendation was made to vectorise the mu funcion, which should speed up the code by quite a bit, so:\n",
        "vector = np.vectorize(mu)\n",
        "#followed by a reccomendation to shorten it even further:\n",
        "vectormu = lambda h, OmegaM, OmegaL: vector(dataframe['redshift'], h, OmegaM, OmegaL)\n",
        "\n",
        "theta = np.array([h, Omega_M, Omega_L])\n",
        "#this to make matters hopefully easier on everybody.\n",
        "\n",
        "#and now this allows for a pretty easy CHI2 function:\n",
        "def chi2(h, OmegaM, OmegaL):\n",
        "    return np.sum(np.square(np.subtract(vectormu(h,OmegaM, OmegaL),dataframe['distance_modulus']))/(np.square(dataframe['distance_modulus_error'])))\n",
        "\n",
        "#this method changes the parameters up a bit so a change can be measured.\n",
        "def new_para(Theta):\n",
        "  Omega_M = Theta[0]\n",
        "  Omega_L = Theta[1]\n",
        "  h = Theta[2]\n",
        "\n",
        "  new_M = np.random.normal(Omega_M, 0.01)\n",
        "  new_L = np.random.normal(Omega_L, 0.01)\n",
        "  new_h = np.random.normal(h, 0.01)\n",
        "  return np.array([new_h, new_M, new_L])\n",
        "\n",
        "#last but not least, a function that can check the old and the new chi2 and make a decision based on this as to whether the new values are kept or not:\n",
        "def accepttance(Chinew, Chiold):\n",
        "  rand = np.random.uniform(0,1) #random value used for the edge cases\n",
        "  if Chinew < Chiold:\n",
        "    return True #new values are better, so we keep them.\n",
        "  elif rand <= min(1,np.exp(-(Chinew - Chiold)/2)) :\n",
        "    return True #new values are within acceptable parameters, albeit not better than the old, so we keep them.\n",
        "  else:\n",
        "   return False #new values are significantly worse, and are rejected.\n",
        "\n",
        "#next we initialise some arrays to read the data from later:\n",
        "accepted = []\n",
        "rejected = []\n",
        "chain = []\n",
        "Numsteps = 580\n",
        "n = 0\n",
        "initsteps = 15 #number of first steps not taken into account.\n",
        "\n",
        "#and then now to get started proper:\n",
        "\n",
        "Chiold = chi2(*theta)\n",
        "\n",
        "with open(filename, 'w') as f:\n",
        "    f.write(\"h\\tOmega_M\\tOmega_L\\tOmegaK\\tchi2\\t\")\n",
        "    while n < Numsteps:\n",
        "        print(f\"step {n}/{Numsteps}\")\n",
        "        # Propose a new set of parameters by generating a random normal distribution around the current parameters \n",
        "        thetanew = new_para(*theta)\n",
        "        # Initialize the chi2 for the proposed parameters\n",
        "        chi2new = chi2(*thetanew)\n",
        "        # Accept or reject the proposed parameters using the function accept_move\n",
        "        if accepttance(chi2current,chi2new):\n",
        "            # the new parameters get copied if it's a valid move\n",
        "            theta = thetanew\n",
        "            # chi2 gets updated\n",
        "            chi2current = chi2new\n",
        "            # this is where the accepted values are actually stored\n",
        "            if n>= initsteps:    \n",
        "                f.write(f\"\\n{theta[0]}\\t{theta[1]}\\t{theta[2]}\\t{1-theta[1]-theta[2]}\\t{chi2current}\\t\")\n",
        "                chain.append(theta)\n",
        "            accepted.append(chi2new)\n",
        "        else:\n",
        "            rejected.append(chi2new) #or we just don't store anything and reject the values.\n",
        "        # we increase the counter by 1 regardless of the quality of the new values, or this never ends.\n",
        "        n += 1 \n",
        "\n",
        "file.close();\n",
        "\n",
        "#there is one very distinct error, which I don't know how to fix, so I gave up after 50 rounds of try and cry. I hope that at least the rest of the code isn't too ridiculous though."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qw52S4G0XccR"
      },
      "source": [
        "After completion of the programme, your MC chain will (*hopefully*) look something like this:\n",
        "\n",
        "`#Omega_m  Omega_L  h`\n",
        "\n",
        "`0.297     0.703    0.698`\n",
        "\n",
        "`0.285     0.715    0.701`\n",
        "\n",
        "`0.288     0.712    0.699`\n",
        "\n",
        "`...`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dB-wD3cLXccR"
      },
      "source": [
        "## <font color='deepskyblue'>Plotting: GetDist</font>\n",
        "\n",
        "We will do the plots with [`GetDist`](https://getdist.readthedocs.io/en/latest/plot_gallery.html).\n",
        "\n",
        "The plot gallery contains a lot of useful examples, you should be able to read them fairly easily and see which applies to your situation. Look for both (triangle) plots and for parameter estimation.\n",
        "\n",
        "Note that many `GetDist` functions require arguments of type `MCSamples`; make sure you convert your lists accordingly.\n",
        "\n",
        "<p>\n",
        "<font color='deepskyblue'>\n",
        "Look at your results. Do you obtain $\\Omega_\\Lambda \\neq 0$ to a high degree of significance? \n",
        "\n",
        "If yes, then my dear friend, you have **DISCOVERED THE ACCELERATED EXPANSION OF THE UNIVERSE AND WON THE NOBEL PRIZE IN PHYSICS 2011!**\n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JIvURq9XccR"
      },
      "outputs": [],
      "source": [
        "# These are just randomly generated values. YOU WILL HAVE TO USE THE RESULTS OF YOUR MCMC CHAIN!\n",
        "h, OmegaM, OmegaL = np.random.normal(0.7, 0.03, N_steps), np.random.normal(0.3, 0.1, N_steps), np.random.normal(0.7, 0.1, N_steps)\n",
        "Omegak = 1. - OmegaM - OmegaL\n",
        "\n",
        "# Here let's just define variable names, labels, and so on\n",
        "my_labels = [r'h', r'\\Omega_m', r'\\Omega_\\Lambda', r'\\Omega_k']\n",
        "my_names = ['h', 'OmegaM', 'OmegaL', 'Omegak']\n",
        "my_dim = 4 # The number of variables we're plotting\n",
        "\n",
        "my_samples = np.array([h, OmegaM, OmegaL, Omegak]) # This creates a higher-dimensional array with all our data,\n",
        "my_samples = np.transpose(np.reshape(my_samples,(my_dim,int(my_samples.size/my_dim)))) # and this reshapes it in a convenient form...\n",
        "samples1 = MCSamples(samples = my_samples, labels = my_labels, names = my_names) # ... to create an \"MCSamples\" data type, which GetDist can use\n",
        "\n",
        "conf_level = 0.6827 # The usual 1-sigma confidence level. You can change to 2- or 3-sigma if you want\n",
        "\n",
        "# Let's print a table with the 1D posteriors (confidence levels)\n",
        "for nam in my_names:\n",
        "    print(round(samples1.confidence(nam,(1.-conf_level)/2.,upper=False),2), \\\n",
        "          \" < \", nam , \" < \", round(samples1.confidence(nam,(1.-conf_level)/2.,upper=True),2),\\\n",
        "          \" at \" , int(100*conf_level),\"\\b% CL\")\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "#Triangle corner plot\n",
        "j = plots.getSubplotPlotter(subplot_size=3)\n",
        "j.settings.axes_labelsize = 22\n",
        "j.settings.axes_fontsize = 16\n",
        "j.triangle_plot(samples1, filled = True, title_limit=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAer6k-yngup"
      },
      "source": [
        "## <font color='deepskyblue'>Further Improvements</font>\n",
        "\n",
        "There are many ways in which you can improve your code and/or your data visualisation. Some ideas, in random order:\n",
        "\n",
        "* Modify your priors and compare your results. For instance:\n",
        "  * Change to log-flat priors for $h$ and/or $\\Omega_m$, namely displace them in log-space instead of linear space:\n",
        "  $$\n",
        "  \\log(h_{\\rm new}) = \\log(h_{\\rm old}) + \\delta\n",
        "  $$\n",
        "  where $\\delta$ a random variable of mean 0\n",
        "  * Use priors from other cosmological probes, for example the [Planck 2018 results](https://arxiv.org/abs/1807.06209):\n",
        "  $$\n",
        "  \\Omega_m = 0.315 \\pm 0.007\\,,~\\dots\n",
        "  $$\n",
        "* Implement a Gelman-Rubin (or alternative) convergence test\n",
        "* Change colours and other properties of your triangle plot\n",
        "* Re-plot the data with your theoretical fits: you can try both your best fit, and some $1\\sigma$ \"error bars\"\n",
        "* Split your data in redshift bins and repeat the analysis for each bin individually.\n",
        "<br> _(What do you notice? Which redshift bins give us the most accurate detection of the cosmological parameters?)_"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Hxy2SP9SXccN"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}